import time

import google.generativeai as genai
import streamlit as st
from vertexai.preview.generative_models import ChatSession, GenerativeModel
from mypylib.streamlit_helper import authenticate, check_and_force_logout
from mypylib.google_gemini import configure

# import vertexai
# from vertexai.preview.generative_models import GenerativeModel, Part
# æ³¨æ„ ï¼šå¯¹äº Gemini æ¨¡å‹ï¼Œä¸€ä¸ªä»¤ç‰Œçº¦ç›¸å½“äº 4 ä¸ªå­—ç¬¦ã€‚100 ä¸ªè¯å…ƒçº¦ä¸º 60-80 ä¸ªè‹±è¯­å•è¯ã€‚
# response.usage_metadata.total_token_count

# region é¡µé¢è®¾ç½®

st.set_page_config(
    page_title="èŠå¤©æœºå™¨äºº",
    page_icon="ğŸ¤–",
    layout="wide",
)

# endregion

# region è¾¹æ 

st.sidebar.markdown(
    """:rainbow[è¿è¡Œè®¾ç½®]\n
ğŸ”¯ æ¨¡å‹ï¼šGemini Pro            
"""
)
sidebar_status = st.sidebar.empty()

# endregion

authenticate(st)
check_and_force_logout(st, sidebar_status)

# def multiturn_generate_content():
#     config = {"max_output_tokens": 2048, "temperature": 0.9, "top_p": 1}
#     model = GenerativeModel("gemini-pro")
#     chat = model.start_chat()
#     response = chat.send_message("""ä½ å¥½""", generation_config=config)
#     # st.write(response.usage_metadata.total_token_count)  # type: ignore
#     st.write(response.text)
#     total_token_count = response._raw_response.usage_metadata
#     st.write(total_token_count.total_token_count)


# model = genai.GenerativeModel("gemini-pro")
# response = model.generate_content("ç”Ÿæ´»çš„æ„ä¹‰æ˜¯ä»€ä¹ˆï¼Ÿ")
# st.markdown(response.text)
